install.packages("e1071")
library(e1071)
library(kernlab)
# Clear the workspace
rm(list = ls())
load('HWD.RData')
X <- as.data.frame(data)
n <- dim(X)[1]
View(X)
View(X)
# Assuming the first column contains the target variable
id <- sample(1:n, floor(0.8 * n))
training <- X[id, ]
test <- X[-id, ]
training$V1 <- as.factor(training$V1)
test$V1 <- as.factor(test$V1)
View(training)
View(training)
?svm
# Using linear SVM model -> svm()
svm_1 <- svm(V1 ~ ., data = training, kernel = 'vanilladot',kpar=list(), C = 0.001)
?ksvm
# Using linear SVM model -> vanilladot
svm_1 <- ksvm(V1 ~ ., data = training, kernel = 'vanilladot',kpar=list(), C = 0.001)
pred <- predict(svm_1, newdata = test[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test$V1)) / length(test$V1)
print(paste("Accuracy:", accuracy))
V1
?cv.ksvm
# Clear the workspace
rm(list = ls())
load('HWD.RData')
X <- as.data.frame(data)
n <- dim(X)[1]
# Assuming the first column contains the target variable
id <- sample(1:n, floor(0.8 * n))
training <- X[id, ]
test <- X[-id, ]
training$V1 <- as.factor(training$V1)
test$V1 <- as.factor(test$V1)
evaluate_model <- function(training, test) {
svm_1 <- ksvm(V1 ~ ., data = training, kernel = 'vanilladot',kpar=list(), C = 0.001)
pred <- predict(svm_1, newdata = test[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test$V1)) / length(test$V1)
return(accuracy)
}
evaluate_model <- function(training_set, test_set) {
svm_1 <- ksvm(V1 ~ ., data = training_set, kernel = 'vanilladot',kpar=list(), C = 0.001)
pred <- predict(svm_1, newdata = test_set[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test_set$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test_set$V1)) / length(test_set$V1)
return(accuracy)
}
accuracy <- evaluate_model(training, test)
# Convert all steps into a function for making cross-validation easier
evaluate_model <- function(training_set, test_set, c) {
# Training the model
svm_1 <- ksvm(V1 ~ ., data = training_set, kernel = 'vanilladot',kpar=list(), C = c)
pred <- predict(svm_1, newdata = test_set[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test_set$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test_set$V1)) / length(test_set$V1)
return(accuracy)
}
rm(list = ls())
load('HWD.RData')
X <- as.data.frame(data)
n <- dim(X)[1]
# Assuming the first column contains the target variable
id <- sample(1:n, floor(0.8 * n))
training <- X[id, ]
test <- X[-id, ]
training$V1 <- as.factor(training$V1)
test$V1 <- as.factor(test$V1)
# Convert all steps into a function for making cross-validation easier
evaluate_model <- function(training_set, test_set, c, my_kernel) {
# Training the model
svm_1 <- ksvm(V1 ~ ., data = training_set, kernel=my_kernel, kpar=list(), C = c)
pred <- predict(svm_1, newdata = test_set[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test_set$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test_set$V1)) / length(test_set$V1)
return(accuracy)
}
accuracy <- evaluate_model(training, test, c=0.001, my_kernel='vanilladot')
print(accuracy)
k_fold <- function(training, k, C) {
# Shuffle indices to create random folds
order <- sample(1:nrow(training))
# Calculate the number of points in each fold
num_points <- round(nrow(training) / k)
# Initialize vector to store RMSE values for each fold
accuracies <- numeric(k)
for (fold in 0:(k - 1)) {
# Determine the start and end indices for the current fold
start_ind = fold * num_points + 1
end_ind <- (fold + 1) * num_points
# Extract indices for the testing set
test_idx <- order[start_ind:end_ind]
# Split the dataset into training and testing sets
test_set <- training[test_idx,]
training_set <- training[-test_idx,]
training_set$V1 <- as.factor(training_set$V1)
test_set$V1 <- as.factor(test_set$V1)
# Evaluate the model
acc <- evaluate_model(training, test, c=C, my_kernel='vanilladot')
accuracies[fold] <- acc
}
return(accuracies)
}
slackness_param <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
param_accuracies <- numeric(length(slackness_param))
for (i in 1:length(slackness_param)) {
ave_acc <- mean(k_fold(training, 10, slackness_param[i]))
param_accuracies[i] <- ave_acc
}
slackness_param <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
param_accuracies <- numeric(length(slackness_param))
for (i in 1:length(slackness_param)) {
cat(i, "/", length(slackness_param), "\r")
ave_acc <- mean(k_fold(training, 10, slackness_param[i]))
param_accuracies[i] <- ave_acc
}
plot(slackness_param, param_accuracies)
best_C <- slacknes_param[which.max(param_accuracies)]
best_C <- slackness_param[which.max(param_accuracies)]
cat("Best slackness parameter was: C =", best_C)
plot(log10(slackness_param), param_accuracies)
plot(log10(slackness_param), param_accuracies);
best_C <- slackness_param[which.max(param_accuracies)];
plot(log10(slackness_param), param_accuracies)
best_C <- slackness_param[which.max(param_accuracies)]
cat("Best slackness parameter was: C =", best_C)
accuracy <- evaluate_model(training, test, c=best_C, my_kernel='vanilladot')
cat("The accuracy of the model on the test set was:", accuracy)
cat("Best slackness parameter was: C =", best_C)
cat("for this value of C, the accuracy was:", max(param_accuracies))
kernel_params <- list('vanilladot', list())
rm(list = ls())
load('HWD.RData')
X <- as.data.frame(data)
n <- dim(X)[1]
# Assuming the first column contains the target variable
id <- sample(1:n, floor(0.8 * n))
training <- X[id, ]
test <- X[-id, ]
training$V1 <- as.factor(training$V1)
test$V1 <- as.factor(test$V1)
evaluate_model <- function(training_set, test_set, c, my_kernel, kpar) {
# Training the model using a chosen kernel and slackness parameter
svm_1 <- ksvm(V1 ~ ., data = training_set, kernel=my_kernel, kpar=kpar, C = c)
pred <- predict(svm_1, newdata = test_set[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test_set$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test_set$V1)) / length(test_set$V1)
return(accuracy)
}
accuracy <- evaluate_model(training, test, c=0.001,
my_kernel='vanilladot', kpar=list())
print(accuracy)
k_fold <- function(training, k, C, kernel, kernel_par) {
# Shuffle indices to create random folds
order <- sample(1:nrow(training))
# Calculate the number of points in each fold
num_points <- round(nrow(training) / k)
# Initialize vector to store RMSE values for each fold
accuracies <- numeric(k)
for (fold in 0:(k - 1)) {
# Determine the start and end indices for the current fold
start_ind = fold * num_points + 1
end_ind <- (fold + 1) * num_points
# Extract indices for the testing set
test_idx <- order[start_ind:end_ind]
# Split the dataset into training and testing sets
test_set <- training[test_idx,]
training_set <- training[-test_idx,]
training_set$V1 <- as.factor(training_set$V1)
test_set$V1 <- as.factor(test_set$V1)
# Evaluate the model
acc <- evaluate_model(training, test, c=C, my_kernel=kernel, kpar=kernel_par)
accuracies[fold] <- acc
}
return(accuracies)
}
slackness_param <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
kernel_params <- list(type='vanilladot', kpar=list())
param_accuracies <- numeric(length(slackness_param))
for (i in 1:length(slackness_param)) {
cat(i, "/", length(slackness_param), "\r")
ave_acc <- mean(k_fold(training, 10, slackness_param[i],
kernel=kernel_params$type, kernel_par=kernel_params$kpar))
param_accuracies[i] <- ave_acc
}
plot(log10(slackness_param), param_accuracies)
best_C <- slackness_param[which.max(param_accuracies)]
cat("Best slackness parameter was: C =", best_C)
# Evaluate the model on the true test set
accuracy <- evaluate_model(training, test, c=best_C,
my_kernel=kernel_params$type, kernel_par=kernel_params$kpar)
# Evaluate the model on the true test set
accuracy <- evaluate_model(training, test, c=best_C,
my_kernel=kernel_params$type, kpar=kernel_params$kpar)
cat("The accuracy of the model on the test set was:", accuracy)
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
sigmas <- c(0.01, 0.1, 0.5, 1)
comb = expand.grid(slackness, sigmas)
comb = expand.grid(C= slackness, sigma=sigmas)
print(comb$C[1])
print(comb$C)
?seq
rm(list = ls())
load('HWD.RData')
X <- as.data.frame(data)
n <- dim(X)[1]
# Assuming the first column contains the target variable
id <- sample(1:n, floor(0.8 * n))
training <- X[id, ]
test <- X[-id, ]
training$V1 <- as.factor(training$V1)
test$V1 <- as.factor(test$V1)
# Convert all steps into a function for making cross-validation easier
evaluate_model <- function(training_set, test_set, c, my_kernel, kpar) {
# Training the model using a chosen kernel and slackness parameter
svm_1 <- ksvm(V1 ~ ., data = training_set, kernel=my_kernel, kpar=kpar, C = c)
pred <- predict(svm_1, newdata = test_set[,-1])
# Convert probabilities to integer predictions
pred_int <- as.integer(factor(pred, levels = levels(test_set$V1)))
# Evaluate accuracy
accuracy <- sum(pred_int == as.integer(test_set$V1)) / length(test_set$V1)
return(accuracy)
}
accuracy <- evaluate_model(training, test, c=0.001,
my_kernel='vanilladot', kpar=list())
print(accuracy)
k_fold <- function(training, kfolds, C, kernel, kpar) {
# Shuffle indices to create random folds
order <- sample(1:nrow(training))
# Calculate the number of points in each fold
num_points <- round(nrow(training) / kfolds)
# Initialize vector to store RMSE values for each fold
accuracies <- numeric(kfolds)
for (fold in 0:(kfolds - 1)) {
# Determine the start and end indices for the current fold
start_ind = fold * num_points + 1
end_ind <- (fold + 1) * num_points
# Extract indices for the testing set
test_idx <- order[start_ind:end_ind]
# Split the dataset into training and testing sets
test_set <- training[test_idx,]
training_set <- training[-test_idx,]
training_set$V1 <- as.factor(training_set$V1)
test_set$V1 <- as.factor(test_set$V1)
# Evaluate the model
acc <- evaluate_model(training, test, c=C, my_kernel=kernel, kpar=kpar)
accuracies[fold] <- acc
}
return(accuracies)
}
# Cross validation in order to obtain the optimal slackness parameter
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
kparams <- list(type='vanilladot', kpar=list())
accuracies <- numeric(length(slackness))
for (i in 1:length(slackness_param)) {
cat(i, "/", length(slackness_param), "\r")
ave_acc <- mean(k_fold(training, 10, slackness[i],
kernel=kparams$type, kernel_par=kparams$kpar))
accuracies[i] <- ave_acc
}
# Cross validation in order to obtain the optimal slackness parameter
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
kparams <- list(type='vanilladot', kpar=list())
accuracies <- numeric(length(slackness))
for (i in 1:length(slackness)) {
cat(i, "/", length(slackness), "\r")
ave_acc <- mean(k_fold(training, 10, slackness[i],
kernel=kparams$type, kernel_par=kparams$kpar))
accuracies[i] <- ave_acc
}
# Cross validation in order to obtain the optimal slackness parameter
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
kparams <- list(type='vanilladot', kpar=list())
accuracies <- numeric(length(slackness))
for (i in 1:length(slackness)) {
cat(i, "/", length(slackness), "\r")
ave_acc <- mean(k_fold(training, 10, slackness[i],
kernel=kparams$type, kpar=kparams$kpar))
accuracies[i] <- ave_acc
}
# Plot the results
plot(log10(slackness), accuracies)
best_C <- slackness[which.max(accuracies)]
cat("Best slackness parameter was: C =", best_C)
# Evaluate the model on the true test set
accuracy <- evaluate_model(training, test, c=best_C,
my_kernel=kparams$type, kpar=kparams$kpar)
cat("The accuracy of the model on the test set was:", accuracy)
seq(11)
1:11
# Task 3.2: Do the same but using a nonlinear kernel
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
sigmas <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
combination = expand.grid(C=slackness, sigma=sigmas)
accuracies <- numeric(nrow(comb))
# Task 3.2: Do the same but using a nonlinear kernel
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
sigmas <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
combination = expand.grid(C=slackness, sigma=sigmas)
accuracies <- numeric(nrow(combination))
for (i in 1:nrow(combination)) {
cat(i, "/", nrow(combination), "\r")
ave_acc <- mean(k_fold(training,
kfolds=10,
C=combination$C[i],
kernel='rbfdot',
kpar=combination$sigma[i]
)
)
accuracies[i] <- ave_acc
}
# Task 3.2: Do the same but using a nonlinear kernel
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
sigmas <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
combination = expand.grid(C=slackness, sigma=sigmas)
accuracies <- numeric(nrow(combination))
for (i in 1:nrow(combination)) {
cat(i, "/", nrow(combination), "\r")
ave_acc <- mean(k_fold(training,
kfolds=10,
C=combination$C[i],
kernel='rbfdot',
kpar=list(combination$sigma[i])
)
)
accuracies[i] <- ave_acc
}
# Task 3.2: Do the same but using a nonlinear kernel
slackness <- c(0.005, 0.1, 1)
sigmas <- c(0.005, 0.1, 1)
combination = expand.grid(C=slackness, sigma=sigmas)
# Here we train using the rbf-kernel
accuracies <- numeric(nrow(combination))
for (i in 1:nrow(combination)) {
cat(i, "/", nrow(combination), "\r")
ave_acc <- mean(k_fold(training,
kfolds=10,
C=combination$C[i],
kernel='rbfdot',
kpar=list(combination$sigma[i])
)
)
accuracies[i] <- ave_acc
}
k_fold <- function(training, kfolds, C, kernel, kpar) {
# Shuffle indices to create random folds
order <- sample(1:nrow(training))
# Calculate the number of points in each fold
num_points <- round(nrow(training) / kfolds)
# Initialize vector to store RMSE values for each fold
accuracies <- numeric(kfolds)
for (fold in 0:(kfolds - 1)) {
cat(fold+1, '\r')
# Determine the start and end indices for the current fold
start_ind = fold * num_points + 1
end_ind <- (fold + 1) * num_points
# Extract indices for the testing set
test_idx <- order[start_ind:end_ind]
# Split the dataset into training and testing sets
test_set <- training[test_idx,]
training_set <- training[-test_idx,]
training_set$V1 <- as.factor(training_set$V1)
test_set$V1 <- as.factor(test_set$V1)
# Evaluate the model
acc <- evaluate_model(training, test, c=C, my_kernel=kernel, kpar=kpar)
accuracies[fold] <- acc
}
return(accuracies)
}
slackness <- c(0.005, 0.1, 1)
sigmas <- c(0.005, 0.1, 1)
combination = expand.grid(C=slackness, sigma=sigmas)
# Here we train using the rbf-kernel
accuracies <- numeric(nrow(combination))
for (i in 1:nrow(combination)) {
cat(i, "/", nrow(combination), "\n")
ave_acc <- mean(k_fold(training,
kfolds=10,
C=combination$C[i],
kernel='rbfdot',
kpar=list(combination$sigma[i])
)
)
accuracies[i] <- ave_acc
}
plot(1:nrow(combination), accuracies)
best <- combination[which.max(accuracies),]
cat("Best slackness parameter was: C =", best)
cat("Best parameters were: C =", best$C, ", sigma =", best$sigma)
#------------------------------------#
#--- Demo of Factor Analysis in R ---#
#------------------------------------#
library(MVN)
library(MASS)
library(mixtools)
library()
data <- read.table("FCourse.txt",head=T)
head(data) # GEO: geology
data <- read.table("Twin.txt",head=T)
head(data) # GEO: geology
# Clear the workspace
rm(list = ls())
#------------------------------------#
#--- Demo of Factor Analysis in R ---#
#------------------------------------#
library(MVN)
library(MASS)
library(mixtools)
data <- read.table("Twin.txt",head=T)
