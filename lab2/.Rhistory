pred <- predict(svm_1, newdata = test[,-1])
conf_matrix <- confusionMatrix(pred, test$V1)
print(conf_matrix)
print(conf_matrix$table)
print(conf_matrix$overall)
print(conf_matrix$positive)
print(conf_matrix$mode)
print(conf_matrix$dots)
print(conf_matrix$table)
print(conf_matrix$overall)
slackness <- c(0.0001, 0.0005, 0.001, 0.005, 0.1, 1)
grid <- expand.grid(C = slackness)
cv <- trainControl(method = "cv",
number = 10,
verboseIter = TRUE
)
# Train the SVM model using the train function
svm <- train(V1 ~ .,
data = training,
method = "svmLinear",
trControl = cv,
tuneGrid = grid)
# Print the trained model
print(svm)
best_C <- svm$bestTune$C
# Test on true test set
svm_best <- ksvm(V1 ~ ., data = training, kernel = 'vanilladot',kpar=list(), C = best_C)
pred <- predict(svm_best, newdata = test[,-1])
conf_matrix <- confusionMatrix(pred, test$V1)
print(conf_matrix)
```{R, warning=FALSE, message=FALSE}
# Visualize the generated data
plot(X, pch = 20, col = y + 2, xlab = "X1", ylab = "X2", asp = 1, cex = 3)
# Function to generate elliptical data
DGP_ellipse <- function(N = 50, seed = 8312){
set.seed(seed)
oval_fun <- function(x, a=1, b=0.5){b * sqrt(1 - (x / a)^2)}
# Generate data for the first oval
x11 = runif(N, -1, 1)
x12 = c(oval_fun(x11[1:(.5 * N)]), -oval_fun(x11[(.5 * N + 1):N])) + rnorm(N, 0, 0.05)
X = cbind(x11, x12)
# Generate data for the second oval
x21 = runif(N, -1.5, 1.5)
x22 = c(oval_fun(x21[1:(.5 * N)], a = 1.5, b = 0.75), -oval_fun(x21[(.5 * N + 1):N], a = 1.5,   b = 0.75)) + rnorm(N, 0, 0.05)
X = rbind(X, cbind(x21, x22))
# Rotate the data
Q = eigen(matrix(c(1, -4, -4, 1), 2, 2))$vectors
X = X %*% Q
# Create the response variable
y = c(rep(1, N), rep(0, N))
d = cbind(y, X)
return(d)
}
# Generate elliptical data with 10 points
N = 10
d = DGP_ellipse(N)
y = d[, 1]
X = d[, -1]
# Visualize the generated data
plot(X, pch = 20, col = y + 2, xlab = "X1", ylab = "X2", asp = 1, cex = 3)
#--- generate the data OVER ---#
#--- tr_te_split ---#
# Randomly sample 20% of the data for testing
id = sample(1:(2 * N), N * 0.2)
X_tr = X[-id, ]
X_te = X[id, ]
y_tr = y[-id]
y_te = y[id]
# Define a kernel function
kernel_function <- function(x1, x2) {
result <- (t(x1) %*% x2)^2
return(result)
}
# Calculate the Gram matrix for the training data set
calculate_gram_matrix <- function(X) {
n <- nrow(X)
gram_matrix <- matrix(0, n, n)
for (i in 1:n) {
for (j in 1:n) {
gram_matrix[i, j] <- kernel_function(X[i, ], X[j, ])
}
}
return(gram_matrix)
}
# Calculate the Gram matrix for the training data set
K = calculate_gram_matrix(X_tr)
N = dim(K)[1]
# Generate the C matrix
C = matrix(1 / N, N, N)
I = diag(1, N)
# Obtains the centralized K* matrix
KC = K - C %*% K - K %*% C + C %*% K %*% C
# Get eigendecomposition of K*
eig = eigen(KC)
# Choose the third eigen vector/PC
PC3_tr = eig$vectors[, 3]
lambda3_tr = eig$values[3]
# Plot the third principal component
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
points(x_new1, 0, pch = 20, col = 5, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(x_new1, 0, pch = 20, col = 5, cex = 3)
points(x_new2, 0, pch = 20, col = 1, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 20, col = 5, cex = 3)
points(x_new2, 0, pch = 20, col = 1, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0.01, pch = 20, col = y_te[1]+2, cex = 3)
points(x_new2, 0.01, pch = 20, col = y_te[2]+2, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0.02, pch = 20, col = y_te[1]+2, cex = 2)
points(x_new2, 0.02, pch = 20, col = y_te[2]+2, cex = 2)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0.02, pch = 20, col = y_te[1]+2, cex = 3)
points(x_new2, 0.02, pch = 20, col = y_te[2]+2, cex = 3)
# Create a plot to display different plotting symbols
plot(1:25, pch = 1:25, cex = 2, col = "blue", main = "Plotting Symbols", xlab = "X-axis", ylab = "Y-axis")
# Add a legend to identify each symbol
legend("topright", legend = 1:25, pch = 1:25, col = "blue", title = "pch Values")
# Create a plot to display different plotting symbols
plot(10:25, pch = 10:25, cex = 2, col = "blue", main = "Plotting Symbols", xlab = "X-axis", ylab = "Y-axis")
# Add a legend to identify each symbol
legend("topright", legend = 10:25, pch = 10:25, col = "blue", title = "pch Values")
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0.02, pch = 17, col = y_te[1]+2, cex = 3)
points(x_new2, 0.02, pch = 17, col = y_te[2]+2, cex = 3)
# Function to generate elliptical data
DGP_ellipse <- function(N = 50, seed = 8312){
set.seed(seed)
oval_fun <- function(x, a=1, b=0.5){b * sqrt(1 - (x / a)^2)}
# Generate data for the first oval
x11 = runif(N, -1, 1)
x12 = c(oval_fun(x11[1:(.5 * N)]), -oval_fun(x11[(.5 * N + 1):N])) + rnorm(N, 0, 0.05)
X = cbind(x11, x12)
# Generate data for the second oval
x21 = runif(N, -1.5, 1.5)
x22 = c(oval_fun(x21[1:(.5 * N)], a = 1.5, b = 0.75), -oval_fun(x21[(.5 * N + 1):N], a = 1.5,   b = 0.75)) + rnorm(N, 0, 0.05)
X = rbind(X, cbind(x21, x22))
# Rotate the data
Q = eigen(matrix(c(1, -4, -4, 1), 2, 2))$vectors
X = X %*% Q
# Create the response variable
y = c(rep(1, N), rep(0, N))
d = cbind(y, X)
return(d)
}
# Generate elliptical data with 10 points
N = 10
d = DGP_ellipse(N)
y = d[, 1]
X = d[, -1]
# Visualize the generated data
plot(X, pch = 20, col = y + 2, xlab = "X1", ylab = "X2", asp = 1, cex = 3)
# Generate elliptical data with 10 points
my_seed <- set.seed(8312)
N = 10
d = DGP_ellipse(N, seed=my_seed)
y = d[, 1]
X = d[, -1]
# Visualize the generated data
plot(X, pch = 20, col = y + 2, xlab = "X1", ylab = "X2", asp = 1, cex = 3)
# Generate elliptical data with 10 points
N = 10
d = DGP_ellipse(N, seed=my_seed)
y = d[, 1]
X = d[, -1]
# Visualize the generated data
plot(X, pch = 20, col = y + 2, xlab = "X1", ylab = "X2", asp = 1, cex = 3)
# Define a kernel function
kernel_function <- function(x1, x2) {
result <- (t(x1) %*% x2)^2
return(result)
}
# Calculate the Gram matrix for the training data set
calculate_gram_matrix <- function(X) {
n <- nrow(X)
gram_matrix <- matrix(0, n, n)
for (i in 1:n) {
for (j in 1:n) {
gram_matrix[i, j] <- kernel_function(X[i, ], X[j, ])
}
}
return(gram_matrix)
}
# Calculate the Gram matrix for the training data set
K = calculate_gram_matrix(X_tr)
N = dim(K)[1]
# Generate the C matrix
C = matrix(1 / N, N, N)
I = diag(1, N)
# Obtains the centralized K* matrix
KC = K - C %*% K - K %*% C + C %*% K %*% C
# Get eigendecomposition of K*
eig = eigen(KC)
# Choose the third eigen vector/PC
PC3_tr = eig$vectors[, 3]
lambda3_tr = eig$values[3]
# Plot the third principal component
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0.02, pch = 17, col = y_te[1]+2, cex = 3)
points(x_new2, 0.02, pch = 17, col = y_te[2]+2, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0.02, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0.02, pch = 17, col = y_te[2]+2, cex = 2)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
# Calculate the Gram matrix for the training data set
K = calculate_gram_matrix(X_tr)
N = dim(K)[1]
# Generate the C matrix
C = matrix(1 / N, N, N)
I = diag(1, N)
# Obtains the centralized K* matrix
KC = K - C %*% K - K %*% C + C %*% K %*% C
# Get eigendecomposition of K*
eig = eigen(KC)
# Choose the third eigen vector/PC
PC3_tr = eig$vectors[, 3]
lambda3_tr = eig$values[3]
# Plot the third principal component
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
legend()
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
legend("Training data", "First test point", "Second test point")
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
legend("topright","Training data", "First test point", "Second test point")
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
legend("topright", legend = c("Training data", "First test point", "Second test point"), pch = c(20, 17, 17), col = c(y + 2, y_te[1] + 2, y_te[2] + 2), cex = 1.2)
# Test on the testing data
# Create a matrix for new observations
X_new = matrix(0, N, 2)
# Generate the vector of the kernel functions
for (obs in 1:dim(X_te)[2]) {
for (i in 1:N) {
X_new[i, obs] = kernel_function(X_tr[i, ], X_te[obs, ])
}
}
# Calculate the new observations
x_new1 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 1] - 1 / N * K %*% rep(1, N))
x_new2 = lambda3_tr^-0.5 * t(PC3_tr) %*% (I - C) %*% (X_new[, 2] - 1 / N * K %*% rep(1, N))
# Plot the new observations
plot(PC3_tr, rep(0, N), pch = 20, col = y + 2, xlab = "Z3", ylab = "", asp = 1, cex = 3)
points(x_new1, 0, pch = 17, col = y_te[1]+2, cex = 2)
points(x_new2, 0, pch = 17, col = y_te[2]+2, cex = 2)
legend("topright", legend = c("Training data", "Test points"), pch = c(20, 17), cex = 1.2)
Just as in task 1, we now define our Gram matrix function and the kernel function as specified in the task as the Radial Basis Function(RBF). After this, we define a function that takes the different hyperparameters $\sigma$, and $\lambda$ and calculates a prediction $\hat y$.
# Load necessary libraries
library(MASS)
library(kernlab)
# Load the Boston dataset
X = data(Boston)
# Define the Gaussian kernel function
kernel_function <- function(x1, x2, sigma) {
result <- exp(-sqrt(sum((x1 - x2)^2)) / (2 * sigma^2))
return(result)
}
# Function to calculate the Gram matrix
calculate_gram_matrix <- function(X, sigma) {
n <- nrow(X)
gram_matrix <- matrix(0, n, n)
for (i in 1:n) {
for (j in 1:n) {
gram_matrix[i, j] <- kernel_function(X[i,], X[j,], sigma)
}
}
return(gram_matrix)
}
# Function to predict using Kernel Ridge Regression
predictKRR <- function(K, trainX, trainY, X, lambda, sigma) {
# Calculate kappa-vector
N = dim(K)[1]
kappa <- matrix(0, nrow(K), 1)
for (i in 1:N) {
kappa[i] = kernel_function(trainX[i,], X, sigma)
}
# Predict y_hat
y_hat = trainY %*% solve(K + diag(lambda, nrow(K))) %*% kappa
return(y_hat)
}
# Divide into training and test sets
set.seed(2020)
train_indices <- sample(1:nrow(Boston), 400)
train_data <- Boston[train_indices, ]
test_data <- Boston[-train_indices, ]
trainY <- train_data$medv
trainX <- train_data[, !(names(train_data) %in% c("medv"))]
testY <- test_data$medv
testX <- test_data[, !(names(test_data) %in% c("medv"))]
trainX <- as.matrix(trainX)
testX <- as.matrix(testX)
# Task 2: Kernel Ridge Regression with specified lambda and sigma
lambda = 0.1
sigma = 0.001
# Calculate the Gram matrix
K = calculate_gram_matrix(trainX, sigma)
# Initialize prediction vector
pred <- matrix(0, length(testY), 1)
# Make predictions for each test data point
for (k in 1:length(testY)) {
pred[k] = predictKRR(K, trainX, trainY, testX[k,], lambda, sigma)
}
# Calculate and print the root mean squared error (RMSE) on the test data
rmse = sqrt(mean((pred - testY)^2))
cat("RMSE on test data:", rmse, "\n")
