{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPLqbOhGyaFE"
   },
   "source": [
    "# Assignment 2, task 2\n",
    "**Xijia Liu, Ume√• University**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXjsJeaazCrH"
   },
   "source": [
    "## Task 2: Nonlinear feature extraction approach, Autoencoder\n",
    "\n",
    "**Background**: In the previous lecture, we learned a nonlinear version of PCA, KPCA, based on feature mappings and kernel tricks. In fact, there is another idea to modify PCA to a nonlinear feature extraction approach based on neural networkd. Indeed, the second formulation of PCA which is based on image reconstruction can be understood as a special neural network that has identical input and output layers. If we add more hidden layers and use a nonlinear activation function, then will come up with the nonlinear model for feature extraction, Autoencoder. \n",
    "\n",
    "**Task description**: in this task, we will implement a simple Autoencoder model with MNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ9uPdIh2n46"
   },
   "source": [
    "### Task 2.1: Data import, visualization, and preprocess\n",
    "\n",
    "Import the MNIST data from Tensorflow and visualize 25 random selected images. In order to reduce the burden of the learning process, we normalize the pixel values such that the values are ranging between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hL-J86zyWDG"
   },
   "outputs": [],
   "source": [
    "import()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYPoxYBM3vBn"
   },
   "source": [
    "### Task 2.2: Build the model\n",
    "\n",
    "Apply the 'sequential' approach to build a simple Autoencoder model. In the encoder part, we add three hidden layers (dense layer) with 128, 64, and 32 neurons respectively. Correspondingly, we include three hidden layers with 64, 128, and 784 neurons respectively in the decoder part. All neurons except the last hidden layer use the Relu function as activation function. Since the all the pixel values have been normalized, all the neurons in the last hidden layer should equip with 'sigmoid' function as activation functions. Once the Autoencoder model is ready, use 'summary' method to visualize the structure of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75F2irSN31Id"
   },
   "outputs": [],
   "source": [
    "# solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE0zyqAH7pk2"
   },
   "source": [
    "### Task 2.3: Compile the model\n",
    "\n",
    "Choose a proper loss function, use 'Adam' optimizer with learing rate $0.01$, and specify 'mse' as performance metric to compile the model define in the previous subtask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5XBW-eg7wV8"
   },
   "outputs": [],
   "source": [
    "# solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHLPNnl27zun"
   },
   "source": [
    "### Task 2.4: Training\n",
    "\n",
    "Train the model with training data 100 epochs. Setting the 'batch_size' as 256 and applying 'shuffle' could be a good idea.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3nycWts74c_"
   },
   "outputs": [],
   "source": [
    "# solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLGFhzcV76xc"
   },
   "source": [
    "### Task 2.5: Visulazing the approximated image \n",
    "\n",
    "Apply 'predict' method to reconstruct the images both in training set and testing set. Randomly visualize 25 reconstructed images from both sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXFTOQDY8GeN"
   },
   "outputs": [],
   "source": [
    "# solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPtAdl7H8IO9"
   },
   "source": [
    "### Task 2.6: Extract the features\n",
    "\n",
    "All the neurons of the last layer in the encoder part can be viewed as extracted features. Please write a program to extract all the feature variables of images both in training set and testing set. \n",
    "\n",
    "**Tips**: Recall how we defined a regression model with pre-determined coefficients in the first example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-tvtU7Z8XmG"
   },
   "outputs": [],
   "source": [
    "# solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZSKfBTl8T3R"
   },
   "source": [
    "### Task 2.7: Train multinomial regression models\n",
    "\n",
    "There are 32 extracted features based on the Autoencoder model and we are going to replace the 784 raw pixel values with them. In other words, we have significantly reduced the dimension of our problem. Although our Autoencoder has only a very simple architecture, the extracted feature is still very useful. Suppose we have a large set of images, however, only 100 observations are annotated, i.e. have target variable (labels). If we use the raw pixel variables as predictors (features) to train a multinomial regression model using the 100 annotated observations, then there must be a high risk of overfitting problems. However, if we train the multinomial regression model using extracted 32 neurons as predictors, then could reduce this risk and improve the generalization ability. \n",
    "\n",
    "**Task**: train two multinomial regression models using the first 100 observations in the training set. The first model uses the 784 raw pixels as input and the second model use the 32 extracted features as inputs. Compare the performance of the two models in the testing set.\n",
    "\n",
    "**Tips**: How to train a multinomial regression in Tensorflow? Let's think about another question instead. Imagine, if we have a binary classification neural network model with no hidden layers, and the activation function of the output layer is the sigmoid function, then what statistical model is this model equivalent to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_IsUWv18jdy"
   },
   "outputs": [],
   "source": [
    "# solutions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNxHH+lWs2aMSLndFb2GkOK",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
