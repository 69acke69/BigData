{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09ehUJkD03Qw"
   },
   "source": [
    "# Assignment 2, task 3\n",
    "**Xijia Liu, Umeå University**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJQEGE_AKW8D"
   },
   "source": [
    "## Task 3: The soul of deep learning, gradient\n",
    "\n",
    "**Background**: deep learning has obtained great success, however, it still has many issues that are unclear. Proper optimization methods to solve the non-convex problem in deep learning is still an open question. In fact, we are empirically using some brute force methods to find the optimal parameter estimation in almost all deep learning applications. In those brute force methods, gradient and its calculation play a key role and therefore it is the soul of deep learning.\n",
    "\n",
    "**Task description**: in this task, we investigate the inner working mechanism of Tensorflow. A BIG remark first. Different from the previous task, We will try to get rid of the bondage of Keras, bypass the \"model compile -> model training\" routine, and use the gradient calculation function of Tensorflow to implement the gradient descent algorithm through our own code.\n",
    "\n",
    "There will be 3 subtasks. First, we learn how to calculate the gradient in Tensorflow. Second, implement the Gradient descent algorithm to a simple numerical problem. Third, apply the Gradient descent algorithm to learn a simple linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6eOsNSOovMe"
   },
   "source": [
    "###Task 3.1 Understand gradient and its calculation in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvArLR4rPBE7"
   },
   "source": [
    "I am from Umeå and skiing is one of our daily basic survival skills. I learned downhill ski through the theory of gradient. So, let's understand gradient by the following gif pitcure.\n",
    "\n",
    "![picture](https://j.gifs.com/y72Q7Y.gif)\n",
    "\n",
    "1. Gradient is a generalization of derivative for a scalar-valued multivariable function.\n",
    "2. Consider $z = f(x, y)$, gradient (vector) of function $z$ is a 2-dim vector of partial derivatives of each varaible. \n",
    "3. $ \\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})^{\\top}$\n",
    "4. The opposite direction of a gradient vector points to the direction of steepest descent. \n",
    "\n",
    "Next, we learn how to evaluate the gradient vector of a multivariable function at certain point. Suppose we want to use tensorflow to calculate the derivative of function \n",
    "$$\n",
    "  f(x) = 2x^2 + 3x + 1\n",
    "$$ \n",
    "at $x_0 = 3$. The derivative can be easily calculated as $f'(x_0) = 4x_0+3 = 15$. How does TF handle this trivial example? Let's see the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 5345,
     "status": "ok",
     "timestamp": 1669457797285,
     "user": {
      "displayName": "Xijia Liu",
      "userId": "07624774260162876472"
     },
     "user_tz": -60
    },
    "id": "Nbxw57s-QgPL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #import TF moudle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "0gW-KdlRP4bm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivative is  15.0\n"
     ]
    }
   ],
   "source": [
    "x_0 = 3.\n",
    "x = tf.Variable(x_0)\n",
    "with tf.GradientTape() as tape:\n",
    "  f = 2*x**2+3*x+1\n",
    "grad = tape.gradient(f, x)\n",
    "print('the derivative is ', float(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQFV6ydC2NNy"
   },
   "source": [
    "The first important thing for evaluating gradient in TensforFlow is define a TensorFlow variable, for example, line 2. Then we can apply function 'GradientTape()' to evaluate the gradient for a function. Here, we can sue Python 'with-as' syntax to define the function and evaluate its gradient efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWqw5YLRPI1i"
   },
   "source": [
    "**Now, it is your turn.** Can you write a few of lines code to calculate the gradient vector of a function of three variables? \n",
    "$$f(x,y,z) = x^3 + 3y^3 + 2z^3 + 2x^2y + 3xyz $$The gradient is $$ \\nabla f = \\begin{pmatrix}\n",
    "3x^2+4xy+3yz\\\\ \n",
    "9y^2+2x^2+3xz\\\\ \n",
    "6z^2+3xy\n",
    "\\end{pmatrix} $$\n",
    "Then your task is apply TF's functions to evaluate the gradient at $(x,y,z)^{T} = (2,4,1)^{T}$.\n",
    "\n",
    "**Tips**: you need to define three tensorflow variables and put them in a array '[x,y,z]' when you apply function 'tape.gradient()' to evaluate the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "gU-pE2SNe9Ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.0,158.0,30.0]\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.)\n",
    "y = tf.Variable(4.)\n",
    "z = tf.Variable(1.)\n",
    "\n",
    "x = [x1,y,z]\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  f = x[0]**3 + 3*x[1]**3 + 2*x[2]**3 + 2*x[0]**2*x[1] + 3*x[0]*x[1]*x[2]\n",
    "\n",
    "grad = tape.gradient(f,x)\n",
    "\n",
    "\n",
    "print(f'[{float(grad[0])},{float(grad[1])},{float(grad[2])}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvceSjY9VVi5"
   },
   "source": [
    "### Task 3.2 Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl7_hgWNVdRJ"
   },
   "source": [
    "Since the negative gradient vector points to the direction of the steepest \n",
    "Suppose we want to minimize an objective function $f(\\textbf{w})$. Since the negative gradient vector, $\\nabla f(\\textbf{w})$, points to the direction of the steepest descent of the objective function, it provides us an idea for finding the minimum value of an objective function. We start from an initial value of the optimizing variable and evaluate the gradient vector at this point, then update the optimizing variable toward the negative gradient direction with a small step, i.e. \n",
    "$$\n",
    "\\textbf{w}_{t+1} = \\textbf{w}_{t} - \\alpha \\nabla f(\\textbf{w}_t)\n",
    "$$ \n",
    "Repeat this procedure many times, then we can find the minimiaer. A one dimensional and a two dimensional example are displayed below.\n",
    "\n",
    "![picture](https://hackernoon.com/hn-images/1*ZmzSnV6xluGa42wtU7KYVA.gif)\n",
    "![picture](https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpL3ukYTWLNk"
   },
   "source": [
    "**Example**: suppose we want to find the minimum value of function $f(x) = x^2-5x-1$ and the minimizer. We can implement the Gradient descent alogrithm as the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1669465064957,
     "user": {
      "displayName": "Xijia Liu",
      "userId": "07624774260162876472"
     },
     "user_tz": -60
    },
    "id": "tPQIRBvSZoFT",
    "outputId": "ac2f0d93-c995-4d63-d79a-6ae4c3651346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4969050884246826\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "x = tf.Variable(0, name=\"variable\", dtype=tf.float32)\n",
    "for i in range (30):\n",
    "  with tf.GradientTape() as tape:\n",
    "    f = x**2-5*x-1\n",
    "  grads = tape.gradient(f, x)\n",
    "  x.assign_sub(lr*grads)\n",
    "print(float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIjUbxpEfO8S"
   },
   "source": [
    "**Remarks**: \n",
    "1. To verify the results, we can find the minimizer by calculating the partial derivative and find the stationary point\n",
    "$$\n",
    "  f'(x) = 2x-5 = 0\n",
    "$$\n",
    "The minimizer is $2.5$\n",
    "2. In the code, line 6, it is the way to update a variable in TensorFlow. Method '.assign_sub' means update 'x' by substract it by 'lr*grads'. You may guess how to update 'x' if we set 'lr = -0.1'\n",
    "3. You also can play this code with different learning rate and summarize the effects of learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wldN85tcZ0UZ"
   },
   "source": [
    "**Now, it is your turn.** In this subtask, you need to write a program to find the minimizer of a bivariable objective function \n",
    "  $$\n",
    "    f(x,y) = (x-1)^2+0.5(y-3)^2\n",
    "  $$\n",
    "**Tips**: \n",
    "1. You can use a while loop with an approximation threshold value, or just simply use a for loop, to find minimizer.\n",
    "2. The initial value can be set as $(x,y)^{\\top} = (0,0)^{\\top}$ and you can choose a proper learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "tAV6O64_kiqj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1.        2.9999204]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "x =  tf.Variable([1.0,0.0], dtype=tf.float32)\n",
    "print(x[0].numpy())\n",
    "\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        f = (x[0] - 1)**2 + 0.5*(x[1]-3)**2 \n",
    "    grads = tape.gradient(f,x)\n",
    "    x.assign_sub(lr*grads)\n",
    "    \n",
    "print(x.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRn9T9HjWMzT"
   },
   "source": [
    "###Task 3.3 Estimate regression coefficients by Gradient Descent algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORzzZjVwsOdv"
   },
   "source": [
    "In this subtask, we apply Gradient descent algorithm to find the least sqaure estimation of regression coefficients. First, let's import some useful tools to our working space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1669461501310,
     "user": {
      "displayName": "Xijia Liu",
      "userId": "07624774260162876472"
     },
     "user_tz": -60
    },
    "id": "ihXUbMMPqNq5"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers, Sequential, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px_ZutBvsx_z"
   },
   "source": [
    "We prepare the data by the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "MV1QLKaEoKd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc99d9ed670>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 30\n",
    "b0 = 2\n",
    "b1 = 2.5\n",
    "x = tf.random.normal([n,1])\n",
    "error = tf.random.normal([n,1], 0, 0.1)\n",
    "y = b0 + b1*x + error\n",
    "plt.scatter(x.numpy(), y.numpy(), marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECWWx2vws3zs"
   },
   "source": [
    "The problem is estimating the regression coefficients from the simulated data using Gradient Descent algorithm. The first thing is write down the objective function and it is the mean square loss\n",
    "  $$\n",
    "    \\frac{1}{n}\\sum_{i=1}^n(y_i - f(x_i))^2\n",
    "  $$\n",
    "where $f(x_i)$ is the regression model. The model we will apply Keras 'Sequential' function, see the codes below. We can see that simple regression model is a special case of a regression Artificial neural network.\n",
    "\n",
    "**The first task of Task 3.3:**\n",
    "\n",
    "In the follwing chunk, you need to write proper codes to complete the defintion of 'model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "_BTIBX9poeCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(units=1, input_dim=1, activation = 'linear')\n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2KMpAbVxJV8"
   },
   "source": [
    "**The second task of Task 3.3**: \n",
    "\n",
    "Now we are ready to implement the Gradient descent algorithm to estimate the regression coefficients. As you can see in the follwing chunck, the code is almost there, but you need to write the line 5 by yourself. \n",
    "\n",
    "**Tips**: In TensorFlow, for the square and mean operator, you can call function 'tf.square' and 'tf.reduce_mean'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "rQzk0ia4ogSy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: [2.0022528]\n",
      "b1: [[2.4788318]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "opt = optimizers.SGD(learning_rate=lr)\n",
    "for i in range(300):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = tf.reduce_mean((tf.square(y-model(x))))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "print('b0:', model.layers[0].bias.numpy())\n",
    "print('b1:', model.layers[0].kernel.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0: [1.988098]\n",
      "b1: [[2.4899855]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "opt = optimizers.SGD(learning_rate=lr)\n",
    "for i in range(300):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = tf.reduce_mean((tf.square(y-model(x))))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  model.layers[0].kernel.assign_sub(lr * grads[0])\n",
    "  model.layers[0].bias.assign_sub(lr * grads[1])\n",
    "print('b0:', model.layers[0].bias.numpy())\n",
    "print('b1:', model.layers[0].kernel.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZJkSjc2qWeN"
   },
   "source": [
    "**The third task of Task 3.3**:\n",
    "\n",
    "As you can see that we use build-in optimization solver in TensorFlow to implement the Gradient Descent algorithm in line 8. Nest, you need to write your own code to replace the 8th line.\n",
    "\n",
    "**Tips**: \n",
    "1. In TensorFlow, the model parameters are stored in different layers. In this case, the slope coefficient is stored in 'model.layers[0].kernel' and the intercept term can be found in 'model.layers[0].bias'. As you can see, in TensorFlow, the weights for each neuron is called 'kernel'. \n",
    "2. Both 'model.layers[0].kernel' and 'model.layers[0].bias' are TensorFlow variables and they have method '.assign_sub' as the example in Task 4.2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN/aV/PN6wAM8IZ296g9Amr",
   "collapsed_sections": [
    "jvArLR4rPBE7"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
